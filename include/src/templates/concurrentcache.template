#ifndef _CACHEPP_CONCURRENTCACHE_TEMPLATE
#define _CACHEPP_CONCURRENTCACHE_TEMPLATE

#include <algorithm>

#include "libs/exceptionpp/exception.h"

#include "src/concurrentcache.h"

template <typename D, typename T> cachepp::ConcurrentCache<D, T>::ConcurrentCache(cachepp::identifier size) : cachepp::CacheInterface<std::map<identifier, std::shared_ptr<T>>, D, T>::CacheInterface() {
	this->size = size;
	for(cachepp::identifier i = 0; i < this->size; ++i) {
		this->cache_l.push_back(std::shared_ptr<std::recursive_mutex> (new std::recursive_mutex()));
	}
}

template <typename D, typename T> void cachepp::ConcurrentCache<D, T>::acquire(const std::shared_ptr<T>& arg, D aux) {
	this->cache_l.at(this->index(arg))->lock();
	this->n_acquire++;
	if(!arg->get_is_loaded()) {
		this->n_miss++;
		this->cache_l.at(this->index(arg))->unlock();
		this->allocate(arg);
	}
	this->access(arg, aux);
}

template <typename D, typename T> void cachepp::ConcurrentCache<D, T>::release(const std::shared_ptr<T>& arg) {
	this->cache_l.at(this->index(arg))->unlock();
}

template <typename D, typename T> void cachepp::ConcurrentCache<D, T>::clear() {
	// fundamentally we need to make this->cache's size atomic -- map.insert and map.erase therefore must be wrapped with a lock
	std::lock_guard<std::recursive_mutex> l (this->l);
	for(cachepp::identifier i = 0; i < this->size; ++i) {
		this->cache_l.at(i)->lock();
	}
	for(typename std::map<cachepp::identifier, std::shared_ptr<T>>::iterator it = this->cache.begin(); it != this->cache.end();) {
		it->second->unload();
		this->cache.erase(it++);
	}
	for(cachepp::identifier i = 0; i < this->size; ++i) {
		this->cache_l.at(i)->unlock();
	}
}

template <typename D, typename T> void cachepp::ConcurrentCache<D, T>::remove(const std::shared_ptr<T>& arg) {
	// fundamentally we need to make this->cache's size atomic -- map.insert and map.erase therefore must be wrapped with a lock
	std::lock_guard<std::recursive_mutex> l (this->l);
	this->cache_l.at(this->index(arg))->lock();
	if(arg->get_is_loaded()) {
		arg->unload();
		this->cache.erase(arg->get_identifier());
	}
	this->cache_l.at(this->index(arg))->unlock();
}

template <typename D, typename T> void cachepp::ConcurrentCache<D, T>::allocate(const std::shared_ptr<T>& arg) {
	// fundamentally we need to make this->cache's size atomic -- map.insert and map.erase therefore must be wrapped with a lock
	std::lock_guard<std::recursive_mutex> l (this->l);

	for(cachepp::identifier i = 0; i < this->cache_l.size(); ++i) {
		this->cache_l.at(i)->lock();
	}

	// competing call may have already allocated this line
	if(arg->get_is_loaded()) {
		for(cachepp::identifier i = 0; i < this->cache_l.size(); ++i) {
			if(i != this->index(arg)) {
				this->cache_l.at(i)->unlock();
			}
		}
		return;
	}

	if(this->cache.size() >= this->size) {
		this->remove(this->select());
	}
	arg->load();
	this->cache.insert(std::pair<cachepp::identifier, std::shared_ptr<T>> (arg->get_identifier(), arg));

	for(cachepp::identifier i = 0; i < this->cache_l.size(); ++i) {
		if(i != this->index(arg)) {
			this->cache_l.at(i)->unlock();
		}
	}
}

template <typename D, typename T> std::shared_ptr<T> cachepp::ConcurrentCache<D, T>::select() {
	bool is_set = false;
	cachepp::identifier heuristic = 0;
	cachepp::identifier target = 0;
	for(typename std::map<identifier, std::shared_ptr<T>>::iterator it = this->cache.begin(); it != this->cache.end(); ++it) {
		cachepp::identifier h = this->heuristic(it->second);
		if(h <= heuristic || !is_set) {
			is_set = true;
			heuristic = h;
			target = it->first;
		}
	}
	if(!is_set) {
		throw(exceptionpp::RuntimeError("cachepp::ConcurrentCache::select", "cannot find a target to evict"));
	}
	return(this->cache.at(target));
}

template <typename D, typename T> bool cachepp::ConcurrentCache<D, T>::in(const std::shared_ptr<T>& arg) {
	throw(exceptionpp::NotImplemented("cachepp::ConcurrentCache::in"));
}

#endif
